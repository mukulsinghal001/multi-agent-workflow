# analysis_code.py

# This file contains the analytical code generated by the LeadAnalyst to test the hypotheses.

from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Initialize Spark Session
spark = SparkSession.builder.appName("CustomerAcquisitionAnalysis").getOrCreate()

# Load mock data based on data_repository_demo.md
# In a real scenario, this would connect to a database or read from files.
customer_data = spark.createDataFrame([
    (1, "Region A", "Segment 1", 35, "High Value"),
    (2, "Region B", "Segment 2", 45, "Medium Value"),
    (3, "Region A", "Segment 1", 25, "Low Value"),
    (4, "Region C", "Segment 3", 55, "High Value"),
    (5, "Region B", "Segment 2", 30, "Medium Value"),
], ["customer_id", "region", "customer_segment", "age", "value_segment"])

sales_transactions = spark.createDataFrame([
    (101, 1, 150.00, "2025-11-20"),
    (102, 2, 200.00, "2025-11-21"),
    (103, 3, 50.00, "2025-11-22"),
    (104, 4, 500.00, "2025-11-23"),
    (105, 5, 250.00, "2025-11-24"),
    (106, 1, 120.00, "2025-12-01"),
], ["transaction_id", "customer_id", "transaction_amount", "transaction_date"])

# --- Analysis for Hypothesis 1 ---
# Analyze customer engagement by region and segment
customer_engagement = customer_data.join(sales_transactions, "customer_id") \
    .groupBy("region", "customer_segment") \
    .agg(
        F.countDistinct("transaction_id").alias("transaction_count"),
        F.sum("transaction_amount").alias("total_spend"),
        F.avg("transaction_amount").alias("avg_transaction_value")
    ) \
    .orderBy("region", F.desc("total_spend"))

print("--- Customer Engagement Analysis (Hypothesis 1) ---")
customer_engagement.show()

# --- Analysis for Hypothesis 2 ---
# Analyze product performance by region
product_sales = spark.createDataFrame([
    (201, "Product A", "Category 1", 1, 150.00),
    (202, "Product B", "Category 2", 2, 200.00),
    (203, "Product A", "Category 1", 3, 50.00),
    (204, "Product C", "Category 1", 4, 500.00),
    (205, "Product B", "Category 2", 5, 250.00),
    (206, "Product A", "Category 1", 1, 120.00),
], ["sale_id", "product_name", "product_category", "customer_id", "sale_amount"])

regional_product_performance = product_sales.join(customer_data, "customer_id") \
    .groupBy("region", "product_category") \
    .agg(
        F.sum("sale_amount").alias("total_sales"),
        F.count("sale_id").alias("units_sold")
    ) \
    .orderBy("region", "product_category")

print("\n--- Regional Product Performance (Hypothesis 2) ---")
regional_product_performance.show()


# --- Analysis for Hypothesis 3 ---
# Identify customers for re-engagement
from pyspark.sql.functions import datediff, current_date, col

# Calculate days since last purchase
customer_last_purchase = sales_transactions.groupBy("customer_id") \
    .agg(F.max("transaction_date").alias("last_purchase_date"))

customers_for_reengagement = customer_last_purchase \
    .withColumn("days_since_last_purchase", datediff(current_date(), col("last_purchase_date")))
    .filter(col("days_since_last_purchase") > 30) \
    .join(customer_data, "customer_id") \
    .select("customer_id", "region", "customer_segment", "days_since_last_purchase")

print("\n--- Customers for Re-engagement (Hypothesis 3) ---")
customers_for_reengagement.show()
